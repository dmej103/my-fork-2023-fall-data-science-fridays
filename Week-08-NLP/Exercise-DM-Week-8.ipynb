{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise with Natural Language Processing\n",
    "\n",
    "For todays exersice we will be doing two things.  The first is to build the same model with the same data that we did in the lecture, the second will be to build a new model with new data. \n",
    "\n",
    "## PART 1: \n",
    "- 20 Newsgroups Corpus\n",
    "0. Inspect data\n",
    "0. Clean and Process Text\n",
    "0. Vectorize your text\n",
    "0. Classify your text using Multinomial Naive Bayes\n",
    "0. Classify your text using Random Forest. \n",
    "0. Eval your models.  \n",
    "0. Classify a NEW PIECE of text. Any string you want to feed it. \n",
    "\n",
    "\n",
    "## PART 2:\n",
    "- Republican vs Democrat Tweet Classifier\n",
    "0.  This is self guided, can you get a f1 above 82%?  -its not easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## IMPORT YOUR LIBS HERE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Text vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#ML helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for helping us with text\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "import nltk \n",
    "# You may need to download these from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display data.\n",
    "1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "1. Print the shape\n",
    "1. Inspect / remove nulls and duplicates\n",
    "1. Find class balances, print out how many of each topic_category there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\r\\...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  topic  \\\n",
       "0   0  From: lerxst@wam.umd.edu (where's my thing)\\r\\...      7   \n",
       "1   1  From: guykuo@carson.u.washington.edu (Guy Kuo)...      4   \n",
       "2   2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4   \n",
       "3   3  From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...      1   \n",
       "4   4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14   \n",
       "\n",
       "          topic_category  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "df = pd.read_csv('data/20-newsgroups.csv')\n",
    "# 2. Print the shape\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "message           0\n",
      "topic             0\n",
      "topic_category    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 3. Inspect / remove nulls and duplicates\n",
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.sport.hockey            600\n",
       "soc.religion.christian      599\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          597\n",
       "sci.crypt                   595\n",
       "rec.autos                   594\n",
       "sci.med                     594\n",
       "comp.windows.x              593\n",
       "sci.space                   593\n",
       "sci.electronics             591\n",
       "comp.os.ms-windows.misc     591\n",
       "comp.sys.ibm.pc.hardware    590\n",
       "misc.forsale                585\n",
       "comp.graphics               584\n",
       "comp.sys.mac.hardware       578\n",
       "talk.politics.mideast       564\n",
       "talk.politics.guns          546\n",
       "alt.atheism                 480\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          377\n",
       "Name: topic_category, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Find class balances, print out how many of each topic_category there are.\n",
    "df.topic_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing \n",
    "(aka Feature engineering)\n",
    "1. Make a function that makes all text lowercase.\n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "2. Make a function that removes all punctuation. \n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "0. EXTRA CREDIT:  \n",
    "    0. Make a function that stemms all words. \n",
    "    0. Make a function that removes all stopwords.\n",
    "\n",
    "5. Mandatory: Make a pipeline function that applys all the text processing functions you just built.\n",
    "    * Do a sanity check by feeding in a test sentence into the pipeline. \n",
    "    \n",
    "6. Mandatory: Use `df['message_clean'] = df[column].apply(???)` and apply the text pipeline to your text data column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sentence with lots of caps.\n"
     ]
    }
   ],
   "source": [
    "# 1. Make a function that makes all text lowercase.\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "\n",
    "\n",
    "test_string = 'This is A SENTENCE with LOTS OF CAPS.'\n",
    "print(make_lower(test_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence 50 With lots of punctuation  other things\n"
     ]
    }
   ],
   "source": [
    "# 2. Make a function that removes all punctuation. \n",
    "def remove_punc(a_string):\n",
    "    a_string = re.sub(r'[^\\w\\s]','', a_string)\n",
    "    return a_string\n",
    "\n",
    "test_string = 'This is a sentence! 50 With lots of punctuation??? & other #things.'\n",
    "print(remove_punc(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence ! With different stopwords added .\n"
     ]
    }
   ],
   "source": [
    "# 3. Make a function that removes all stopwords.\n",
    "def remove_stopwords(a_string):\n",
    "    words = word_tokenize(a_string)\n",
    "    valid_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            valid_words.append(word)\n",
    "    a_string = ' '.join(valid_words)\n",
    "    return a_string\n",
    "\n",
    "test_string = 'This is a sentence! With some different stopwords i have added in here.'\n",
    "print(remove_stopwords(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EXTRA CREDIT: Make a function that stemms all words. \n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'played started playing players love play plays'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. MANDATORY: Make a pipeline function that applys all the text processing functions you just built.\n",
    "def text_pipeline(input_string):\n",
    "    input_string = make_lower(input_string)\n",
    "    input_string = remove_punc(input_string)\n",
    "    input_string = remove_stopwords(input_string)\n",
    "    return input_string\n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n",
    "text_pipeline(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_category</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\r\\...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>lerxstwamumdedu wheres thing subject car nntpp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>guykuocarsonuwashingtonedu guy kuo subject si ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>twillisececnpurdueedu thomas e willis subject ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>jgreenamber joe green subject weitek p9000 org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>jcmheadcfaharvardedu jonathan mcdowell subject...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  topic  \\\n",
       "0   0  From: lerxst@wam.umd.edu (where's my thing)\\r\\...      7   \n",
       "1   1  From: guykuo@carson.u.washington.edu (Guy Kuo)...      4   \n",
       "2   2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...      4   \n",
       "3   3  From: jgreen@amber (Joe Green)\\r\\nSubject: Re:...      1   \n",
       "4   4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...     14   \n",
       "\n",
       "          topic_category                                      message_clean  \n",
       "0              rec.autos  lerxstwamumdedu wheres thing subject car nntpp...  \n",
       "1  comp.sys.mac.hardware  guykuocarsonuwashingtonedu guy kuo subject si ...  \n",
       "2  comp.sys.mac.hardware  twillisececnpurdueedu thomas e willis subject ...  \n",
       "3          comp.graphics  jgreenamber joe green subject weitek p9000 org...  \n",
       "4              sci.space  jcmheadcfaharvardedu jonathan mcdowell subject...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Mandatory: Use `df[column].apply(???)` and apply the text pipeline to your text data column. \n",
    "#df1 = df['message'].apply(text_pipeline)\n",
    "df['message_clean'] = df['message'].apply(text_pipeline)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "1. Define your `X` and `y` data. \n",
    "\n",
    "\n",
    "2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "    * Do you want to use n-grams..?\n",
    "\n",
    "\n",
    "3. Fit your vectorizer using your X data.\n",
    "    * Remember, this process happens IN PLACE.\n",
    "\n",
    "\n",
    "4. Transform your X data using your fitted vectorizer. \n",
    "    * `X = vectorizer.???`\n",
    "\n",
    "\n",
    "\n",
    "5. Print the shape of your X.  How many features (aka columns) do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Define your `X` and `y` data. \n",
    "X = df['message_clean']\n",
    "y = df['topic_category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 5) 9051\n"
     ]
    }
   ],
   "source": [
    "# 1. Train test split your data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Saves raw text for later incase\n",
    "X_train_text = X_train\n",
    "X_test_text = X_test\n",
    "\n",
    "print(df.shape, len(X_train_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msmooth_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "Equivalent to :class:`CountVectorizer` followed by\n",
      ":class:`TfidfTransformer`.\n",
      "\n",
      "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input : {'filename', 'file', 'content'}, default='content'\n",
      "    If 'filename', the sequence passed as an argument to fit is\n",
      "    expected to be a list of filenames that need reading to fetch\n",
      "    the raw content to analyze.\n",
      "\n",
      "    If 'file', the sequence items must have a 'read' method (file-like\n",
      "    object) that is called to fetch the bytes in memory.\n",
      "\n",
      "    Otherwise the input is expected to be a sequence of items that\n",
      "    can be of type string or byte.\n",
      "\n",
      "encoding : str, default='utf-8'\n",
      "    If bytes or files are given to analyze, this encoding is used to\n",
      "    decode.\n",
      "\n",
      "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "    Instruction on what to do if a byte sequence is given to analyze that\n",
      "    contains characters not of the given `encoding`. By default, it is\n",
      "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "    values are 'ignore' and 'replace'.\n",
      "\n",
      "strip_accents : {'ascii', 'unicode'}, default=None\n",
      "    Remove accents and perform other character normalization\n",
      "    during the preprocessing step.\n",
      "    'ascii' is a fast method that only works on characters that have\n",
      "    an direct ASCII mapping.\n",
      "    'unicode' is a slightly slower method that works on any characters.\n",
      "    None (default) does nothing.\n",
      "\n",
      "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "    :func:`unicodedata.normalize`.\n",
      "\n",
      "lowercase : bool, default=True\n",
      "    Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "preprocessor : callable, default=None\n",
      "    Override the preprocessing (string transformation) stage while\n",
      "    preserving the tokenizing and n-grams generation steps.\n",
      "    Only applies if ``analyzer is not callable``.\n",
      "\n",
      "tokenizer : callable, default=None\n",
      "    Override the string tokenization step while preserving the\n",
      "    preprocessing and n-grams generation steps.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "    Whether the feature should be made of word or character n-grams.\n",
      "    Option 'char_wb' creates character n-grams only from text inside\n",
      "    word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "    If a callable is passed it is used to extract the sequence of features\n",
      "    out of the raw, unprocessed input.\n",
      "\n",
      "    .. versionchanged:: 0.21\n",
      "\n",
      "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "    first read from the file and then passed to the given callable\n",
      "    analyzer.\n",
      "\n",
      "stop_words : {'english'}, list, default=None\n",
      "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "    list is returned. 'english' is currently the only supported string\n",
      "    value.\n",
      "    There are several known issues with 'english' and you should\n",
      "    consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "    If a list, that list is assumed to contain stop words, all of which\n",
      "    will be removed from the resulting tokens.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    If None, no stop words will be used. max_df can be set to a value\n",
      "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "    words based on intra corpus document frequency of terms.\n",
      "\n",
      "token_pattern : str\n",
      "    Regular expression denoting what constitutes a \"token\", only used\n",
      "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "    or more alphanumeric characters (punctuation is completely ignored\n",
      "    and always treated as a token separator).\n",
      "\n",
      "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "    The lower and upper boundary of the range of n-values for different\n",
      "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "    only bigrams.\n",
      "    Only applies if ``analyzer is not callable``.\n",
      "\n",
      "max_df : float or int, default=1.0\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly higher than the given threshold (corpus-specific\n",
      "    stop words).\n",
      "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "    documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "min_df : float or int, default=1\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly lower than the given threshold. This value is also\n",
      "    called cut-off in the literature.\n",
      "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "    of documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "max_features : int, default=None\n",
      "    If not None, build a vocabulary that only consider the top\n",
      "    max_features ordered by term frequency across the corpus.\n",
      "\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "vocabulary : Mapping or iterable, default=None\n",
      "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "    indices in the feature matrix, or an iterable over terms. If not\n",
      "    given, a vocabulary is determined from the input documents.\n",
      "\n",
      "binary : bool, default=False\n",
      "    If True, all non-zero term counts are set to 1. This does not mean\n",
      "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "    is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "\n",
      "dtype : dtype, default=float64\n",
      "    Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "norm : {'l1', 'l2'}, default='l2'\n",
      "    Each output row will have unit norm, either:\n",
      "    * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "    similarity between two vectors is their dot product when l2 norm has\n",
      "    been applied.\n",
      "    * 'l1': Sum of absolute values of vector elements is 1.\n",
      "    See :func:`preprocessing.normalize`.\n",
      "\n",
      "use_idf : bool, default=True\n",
      "    Enable inverse-document-frequency reweighting.\n",
      "\n",
      "smooth_idf : bool, default=True\n",
      "    Smooth idf weights by adding one to document frequencies, as if an\n",
      "    extra document was seen containing every term in the collection\n",
      "    exactly once. Prevents zero divisions.\n",
      "\n",
      "sublinear_tf : bool, default=False\n",
      "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "vocabulary_ : dict\n",
      "    A mapping of terms to feature indices.\n",
      "\n",
      "fixed_vocabulary_: bool\n",
      "    True if a fixed vocabulary of term to indices mapping\n",
      "    is provided by the user\n",
      "\n",
      "idf_ : array of shape (n_features,)\n",
      "    The inverse document frequency (IDF) vector; only defined\n",
      "    if ``use_idf`` is True.\n",
      "\n",
      "stop_words_ : set\n",
      "    Terms that were ignored because they either:\n",
      "\n",
      "      - occurred in too many documents (`max_df`)\n",
      "      - occurred in too few documents (`min_df`)\n",
      "      - were cut off by feature selection (`max_features`).\n",
      "\n",
      "    This is only available if no vocabulary was given.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "    matrix of counts.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The ``stop_words_`` attribute can get large and increase the model size\n",
      "when pickling. This attribute is provided only for introspection and can\n",
      "be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      ">>> corpus = [\n",
      "...     'This is the first document.',\n",
      "...     'This document is the second document.',\n",
      "...     'And this is the third one.',\n",
      "...     'Is this the first document?',\n",
      "... ]\n",
      ">>> vectorizer = TfidfVectorizer()\n",
      ">>> X = vectorizer.fit_transform(corpus)\n",
      ">>> print(vectorizer.get_feature_names())\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      ">>> print(X.shape)\n",
      "(4, 9)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "TfidfVectorizer?\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Fit your vectorizer using your X data\n",
    "vectorizer.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform your X data using your fitted vectorizer. \n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 121888) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# 5. Print the shape of your X.  How many features (aka columns) do you have?\n",
    "print(X_train.shape, type(X_train))\n",
    "\n",
    "# I have 121,888 columns.\n",
    "# This is slightly less than what was returned in lecture file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Build and Train Model\n",
    "Use Multinomial Naive Bayes to classify these documents. \n",
    "\n",
    "1. Initalize an empty model. \n",
    "2. Fit the model with our training data.\n",
    "\n",
    "\n",
    "Experiment with different alphas.  Use the alpha gives you the best result.\n",
    "\n",
    "EXTRA CREDIT:  Use grid search to programmatically do this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize an empty model. \n",
    "model = MultinomialNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit our model with our training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model.\n",
    "\n",
    "1. Make new predicitions using our test data. \n",
    "2. Print the accuracy of the model. \n",
    "3. Print the confusion matrix of our predictions. \n",
    "4. Using `classification_report` print the evaluation results for all the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make new predictions of our testing data. \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.877154\n"
     ]
    }
   ],
   "source": [
    "# 2. Print the accuracy of the model. \n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "# Should print 0.877\n",
    "print(\"Model Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConfusionMatrixDisplay' has no attribute 'from_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-943c9f70248d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# you can use Sklearns `ConfusionMatrixDisplay`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConfusionMatrixDisplay' has no attribute 'from_estimator'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1512x1512 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Plot the confusion matrix of our predictions\n",
    "# you can use Sklearns `ConfusionMatrixDisplay`\n",
    "fig, ax = plt.subplots(figsize=(21, 21))\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, ax=ax);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.75      0.82        96\n",
      "           comp.graphics       0.87      0.89      0.88       117\n",
      " comp.os.ms-windows.misc       0.85      0.86      0.85       118\n",
      "comp.sys.ibm.pc.hardware       0.72      0.80      0.76       118\n",
      "   comp.sys.mac.hardware       0.93      0.90      0.92       115\n",
      "          comp.windows.x       0.91      0.89      0.90       119\n",
      "            misc.forsale       0.85      0.68      0.76       117\n",
      "               rec.autos       0.91      0.92      0.92       119\n",
      "         rec.motorcycles       0.97      0.97      0.97       120\n",
      "      rec.sport.baseball       0.97      0.95      0.96       119\n",
      "        rec.sport.hockey       0.92      1.00      0.96       120\n",
      "               sci.crypt       0.82      0.98      0.90       119\n",
      "         sci.electronics       0.92      0.83      0.88       118\n",
      "                 sci.med       0.97      0.96      0.97       119\n",
      "               sci.space       0.95      0.96      0.95       119\n",
      "  soc.religion.christian       0.63      0.98      0.77       120\n",
      "      talk.politics.guns       0.84      0.97      0.90       109\n",
      "   talk.politics.mideast       0.93      0.98      0.96       113\n",
      "      talk.politics.misc       1.00      0.74      0.85        93\n",
      "      talk.religion.misc       0.95      0.24      0.38        75\n",
      "\n",
      "                accuracy                           0.88      2263\n",
      "               macro avg       0.89      0.86      0.86      2263\n",
      "            weighted avg       0.89      0.88      0.87      2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Using `classification_report` print the evaluation results for all the classes. \n",
    "print(classification_report(y_test, y_pred, target_names=model.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual predicition\n",
    "Write a new sentence that you think will be classified as talk.politics.guns. \n",
    "1. Apply the text pipeline to your sentence\n",
    "2. Transform your cleaned text using the `X = vectorizer.transform([your_text])`\n",
    "    * Note, the `transform` function accepts a list and not a individual string.\n",
    "3. Use the model to predict your new `X`. \n",
    "4. Print the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 11314]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-1dad4094894f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3. Use the model to predict your new `X`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# 4. Print the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0;34mf\"Got {alpha.shape[0]} elements instead of {self.n_features_in_}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 )\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0;31m# check that all alpha are positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0malpha_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All values in alpha must be greater than 0.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mX_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mreset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0mreset\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfeature_names_in_\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mchecked\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconsistency\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mreset\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             ]\n\u001b[1;32m     72\u001b[0m             \u001b[0margs_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             warnings.warn(\n\u001b[0m\u001b[1;32m     74\u001b[0m                 (\n\u001b[1;32m     75\u001b[0m                     \u001b[0;34mf\"Pass {args_msg} as keyword args. From version \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         if (\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# is numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F_CONTIGUOUS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 11314]"
     ]
    }
   ],
   "source": [
    "my_sentence = \"There are a lot of illegal guns on the street right now. The mayor better do something about it.\"\n",
    "\n",
    "# 1. Apply the text pipeline to your sentence\n",
    "text_pipeline(my_sentence)\n",
    "\n",
    "# 2. Transform your clean[ed text using the `X = vectorizer.transform([your_text])`\\\n",
    "X = vectorizer.transform([my_sentence])\n",
    "y = df['topic_category']\n",
    "# 3. Use the model to predict your new `X`. \n",
    "model.fit(X, y)\n",
    "# 4. Print the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# PART 2: Twitter Data\n",
    "This part of the exercise is un-guided on purpose.  \n",
    "\n",
    "Using the `dem-vs-rep-tweets.csv` build a classifier to determine if a tweet was written by a democrat or republican. \n",
    "\n",
    "Can you get an f1-score higher than %82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "# 2. Print the shape\n",
    "df = pd.read_csv('data/dem-vs-rep-tweets.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
